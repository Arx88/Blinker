---
description: A integração entre o serviço LLM e o sistema de agentes é fundamental para a operação do sistema, permitindo que os agentes utilizem modelos de linguagem para processamento e geração de conteúdo.
globs: 
alwaysApply: false
---
# Integração LLM com Sistema de Agentes

A integração entre o serviço LLM e o sistema de agentes é fundamental para a operação do sistema, permitindo que os agentes utilizem modelos de linguagem para processamento e geração de conteúdo.

## Pontos de Integração

1. **Módulo de API dos Agentes**
   - O arquivo [agent/api.py](mdc:backend/agent/api.py) importa e utiliza o serviço LLM
   - Utiliza o LLM para:
     - Análise de mensagens dos usuários
     - Geração de respostas
     - Tomada de decisões sobre ferramentas a utilizar

2. **Módulo de Thread Manager**
   - O [agentpress/thread_manager.py](mdc:backend/agentpress/thread_manager.py) integra com o LLM
   - Gerencia conversas completas e contexto
   - Fornece histórico de mensagens formatadas para o LLM

3. **Módulo de Context Manager**
   - O [agentpress/context_manager.py](mdc:backend/agentpress/context_manager.py) gerencia o contexto para o LLM
   - Otimiza o uso de tokens
   - Aplica estratégias de compressão para contextos longos

## Fluxo de Dados

1. **Recebimento de Mensagem**
   ```
   Usuário → API → Thread Manager → Formatação de Contexto → LLM
   ```

2. **Processamento da Resposta**
   ```
   LLM → Extração de Ações → Execução de Ferramentas → Atualização de Contexto → Nova Chamada LLM (se necessário)
   ```

3. **Geração de Resposta Final**
   ```
   LLM → Thread Manager → Armazenamento no Banco → Envio ao Usuário
   ```

## Formatação de Prompts para Agentes

1. **Estrutura do Prompt**
   - Definida em [agent/prompt.py](mdc:backend/agent/prompt.py)
   - Composta por:
     - Instruções do sistema
     - Descrições de ferramentas disponíveis
     - Histórico de conversa
     - Mensagem atual do usuário

2. **Otimizações Específicas**
   - Formato XML para function calling com Claude
   - Agrupamento de mensagens para economia de tokens
   - Resumo de histórico para conversas longas

## Protocolos de Comunicação

1. **Chamada de Ferramenta (Tool Call)**
   ```xml
   <tool_call>
     <tool_name>nome-da-ferramenta</tool_name>
     <parameters>
       <param1>valor1</param1>
       <param2>valor2</param2>
     </parameters>
   </tool_call>
   ```

2. **Resposta de Ferramenta (Tool Response)**
   ```xml
   <tool_result>
     <result>Conteúdo do resultado da ferramenta</result>
   </tool_result>
   ```

3. **Resposta Final ao Usuário**
   ```xml
   <answer>
     Resposta formatada para o usuário baseada no contexto
     e nos resultados das ferramentas.
   </answer>
   ```

## Gerenciamento de Estado

1. **Conversas Multi-turno**
   - O sistema mantém o estado da conversa entre chamadas
   - Cada mensagem incrementa o contexto
   - O LLM tem acesso ao histórico completo (sujeito a limitações de tokens)

2. **Cache e Otimização**
   - Mensagens do sistema são marcadas como ephemeral para Claude
   - Histórico comprimido para economizar tokens
   - Variáveis de contexto são mantidas entre turnos

## Requisitos de Modelo

1. **Capacidades Mínimas**
   - Suporte a function calling / ferramentas
   - Capacidade de seguir instruções precisas
   - Janela de contexto suficiente (mínimo 8k tokens)

2. **Modelos Recomendados**
   - Claude 3 Sonnet/Opus para melhor qualidade
   - GPT-4 para raciocínio complexo
   - Claude 3 Haiku para melhor performance/custo

## Tratamento de Erros Específicos

1. **Erros de Compreensão**
   - Quando o LLM não segue o formato esperado
   - Implementação de tentativas com instruções mais claras
   - Fallback para prompt simplificado

2. **Erros de Execução de Ferramenta**
   - Comunicação clara do erro ao LLM
   - Oportunidade para o LLM tentar abordagem alternativa
   - Registro detalhado para debugging

